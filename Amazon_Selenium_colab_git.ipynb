{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanakaDushmantha/Amazon-Review-Scraping/blob/main/Amazon_Selenium_colab_git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKYw7Tw_IL1c"
      },
      "outputs": [],
      "source": [
        "%pip install selenium\n",
        "%pip install pandas\n",
        "%pip install bs4"
      ],
      "id": "lKYw7Tw_IL1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fd5aea",
      "metadata": {
        "id": "24fd5aea"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from selenium.webdriver import Firefox\n",
        "import pandas as pd\n",
        "import re\n",
        "import traceback\n",
        "from selenium import webdriver\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_XOeJ4UIL1l"
      },
      "outputs": [],
      "source": [
        "filename = 'amazon_urls.txt'\n",
        "file = open('amazon_urls.txt')\n",
        "LIST=re.split(\"\\r\\n|\\n\",file.read())\n",
        "file.close()\n",
        "x = []\n",
        "for i in LIST:\n",
        "    print(i.split('~')[-1])\n",
        "    x.append(i.split('~')[-1])"
      ],
      "id": "z_XOeJ4UIL1l"
    },
    {
      "cell_type": "code",
      "source": [
        "# -- to avoid captcha run amazon url two times and use same driver again and again--\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--disable-gpu')\n",
        "driver = webdriver.Firefox(options=options)"
      ],
      "metadata": {
        "id": "_03Zo05JUOfl"
      },
      "id": "_03Zo05JUOfl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial testing round\n",
        "driver.get(\"https://www.amazon.com/Tomorrow-smash-hit-Sunday-Times-bestseller/dp/152911554X/ref=zg_bs_g_269678_sccl_1/260-6217822-9204433?psc=1\")\n",
        "display(Image(driver.get_full_page_screenshot_as_png()))"
      ],
      "metadata": {
        "id": "6JcMQbhMUgX3"
      },
      "id": "6JcMQbhMUgX3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlWUfjB4IL1o"
      },
      "outputs": [],
      "source": [
        "reviews,reviewers, reviewtitles,review_date,reviewerplace,reviewratings, dfs = [],[],[],[],[],[],[]\n",
        "product_names, prices, auther_names = [],[],[]\n",
        "\n",
        "l = 0\n",
        "while l < len(x):\n",
        "#   url = 'https://www.amazon.com/Bumble-Building-Repair-Styling-Cream/dp/B08V8HH6JS'\n",
        "    url = x[l]\n",
        "\n",
        "    #extract product name from url\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        product_name = url.split(\"/\")[3].replace(\"-\",\" \")\n",
        "        print(product_name, end='\\n')\n",
        "\n",
        "    except Exception as ex:\n",
        "        product_name = \"\"\n",
        "        traceback.print_exc()\n",
        "\n",
        "    #extract product price\n",
        "    try:\n",
        "        price_tag = driver.find_elements(By.CSS_SELECTOR, '#corePrice_feature_div')\n",
        "        print(price_tag[0].text)\n",
        "        price = price_tag[0].text\n",
        "\n",
        "        if price_tag[0].text=='':\n",
        "            price_tag1 = driver.find_elements(By.CSS_SELECTOR, '#newBooksSingleBuyingOptionHeader_feature_div')\n",
        "            print(price_tag1[0].text.split(\"\\n\")[1])\n",
        "            price = price_tag1[0].text.split(\"\\n\")[1]\n",
        "\n",
        "    except Exception as ex:\n",
        "        price = \"\"\n",
        "        traceback.print_exc()\n",
        "        display(Image(driver.get_full_page_screenshot_as_png()))\n",
        "\n",
        "    #extract auther name\n",
        "    try:\n",
        "        auther = driver.find_elements(By.CSS_SELECTOR, '#bylineInfo')\n",
        "        print(auther[0].text.replace(\"by \",\"\").replace(\" (Author)\",\"\"))\n",
        "        auther_name = auther[0].text.replace(\"by \",\"\").replace(\" (Author)\",\"\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        auther_name = \"\"\n",
        "        traceback.print_exc()\n",
        "\n",
        "    try:\n",
        "        all_reviews = driver.find_element(By.CSS_SELECTOR, '#reviews-medley-footer .a-text-bold')\n",
        "        all_reviews.click()\n",
        "    except Exception as ex:\n",
        "        traceback.print_exc()\n",
        "        display(Image(driver.get_full_page_screenshot_as_png()))\n",
        "        l+=1\n",
        "        continue\n",
        "\n",
        "    #scrape review data\n",
        "    while True:\n",
        "        #ratings = driver.find_elements(By.XPATH, '//i[@data-hook=\"review-star-rating\"]/span')\n",
        "        #lst_of_ratings = driver.find_elements(By.CLASS_NAME, \"review-rating\")\n",
        "        #is_verified = driver.find_elements(By.XPATH, '//span[@data-hook=\"avp-badge\"]')\n",
        "        loc_date = driver.find_elements(By.XPATH, \"//span[@data-hook='review-date']\")\n",
        "\n",
        "        rating1 = driver.find_elements(By.XPATH, \"//i[@data-hook='review-star-rating']/span\")\n",
        "        rating2 = driver.find_elements(By.XPATH, \"//i[@data-hook='cmps-review-star-rating']/span\")\n",
        "        rating = rating1 + rating2\n",
        "\n",
        "        review_title1 = driver.find_elements(By.XPATH, \"//a[@data-hook='review-title']/span[2]\")\n",
        "        review_title2 = driver.find_elements(By.XPATH, \"//span[@data-hook='review-title']/span\")\n",
        "        review_title = review_title1 + review_title2\n",
        "\n",
        "        review = driver.find_elements(By.XPATH, '//span[@data-hook=\"review-body\"]')\n",
        "\n",
        "        reviewer1 = driver.find_elements(By.CSS_SELECTOR, '#cm_cr-review_list .a-profile-name')\n",
        "        reviewer = [rev.text.strip() for rev in reviewer1 if rev.text.strip()]\n",
        "\n",
        "        max_value = max(len(loc_date), len(rating), len(review_title), len(review), len(reviewer))\n",
        "        print(max_value)\n",
        "        for i in range(max_value):\n",
        "            try:\n",
        "                other, date = loc_date[i].text.split('on')\n",
        "                match = re.search(r'in\\s+(.*?)\\s+on', loc_date[i].text)\n",
        "                place = match.group(1) if match else \"\"\n",
        "            except:\n",
        "                place, date = \"\", \"\"\n",
        "                pass\n",
        "\n",
        "            review_date.append(date)\n",
        "            reviewerplace.append(place)\n",
        "\n",
        "            try:\n",
        "                reviewratings.append(rating[i].get_attribute('textContent'))\n",
        "            except Exception as ex:\n",
        "                traceback.print_exc()\n",
        "                reviewratings.append(\"\")\n",
        "\n",
        "            try:\n",
        "                reviewtitles.append(review_title[i].text)\n",
        "            except Exception as ex:\n",
        "                reviewtitles.append(\"\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "            try:\n",
        "                reviews.append(review[i].text)\n",
        "            except Exception as ex:\n",
        "                reviews.append(\"\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "            try:\n",
        "                reviewers.append(reviewer[i])\n",
        "            except Exception as ex:\n",
        "                reviewers.append(\"\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "            product_names.append(product_name)\n",
        "            prices.append(price)\n",
        "            auther_names.append(auther_name)\n",
        "\n",
        "        try:\n",
        "            next_btn = driver.find_element(By.XPATH, \"//ul[@class='a-pagination']/li[2]/a\")\n",
        "            next_btn.click()\n",
        "            print(\"goto next page\")\n",
        "            time.sleep(5)\n",
        "\n",
        "        except Exception as ex:\n",
        "            print(\"next page is not found\")\n",
        "            break\n",
        "\n",
        "    l+=1\n",
        "\n",
        "final_data = {'Product':product_names,'Auther':auther_names,'Price':prices,'Review Title':reviewtitles,'Reviews':reviews,'Reviewer':reviewers,\n",
        "    'Date':review_date,'Place':reviewerplace,'Rating':reviewratings}\n",
        "min_length = len(reviewtitles)\n",
        "final_df = pd.DataFrame({k:pd.Series(v[:min_length]) for k,v in final_data.items()})\n",
        "final_df.to_csv('Amazon_review_data.csv',index=False)\n",
        "del final_df\n",
        "# driver.quit()\n",
        "\n",
        "print(\"Finished...\")"
      ],
      "id": "ZlWUfjB4IL1o"
    },
    {
      "cell_type": "code",
      "source": [
        "# script for merge csv files\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set the directory to the current working directory\n",
        "csv_directory = '/content'\n",
        "\n",
        "# Get a list of all CSV files in the directory\n",
        "csv_files = [file for file in os.listdir(csv_directory) if file.endswith('.csv')]\n",
        "\n",
        "# Initialize an empty DataFrame to store the merged data\n",
        "merged_data = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file and merge the data\n",
        "for csv_file in csv_files:\n",
        "    file_path = os.path.join(csv_directory, csv_file)\n",
        "    df = pd.read_csv(file_path, encoding='latin-1')\n",
        "\n",
        "    # Skip merging headers for all files except the first one\n",
        "    if not merged_data.empty:\n",
        "        df = df[1:]\n",
        "\n",
        "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
        "\n",
        "# Write the merged data to a new CSV file\n",
        "merged_data.to_csv('/content/Amazon_review_merged_data.csv', index=False)"
      ],
      "metadata": {
        "id": "UrG_l-jBz6l4"
      },
      "id": "UrG_l-jBz6l4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}